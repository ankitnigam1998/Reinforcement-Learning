{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "class Actor:\n",
        "    def __init__(self, state_dim, action_dim, learning_rate):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "\n",
        "    def create_model(self):\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(24, activation='relu', input_shape=(self.state_dim,)),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_dim, activation='softmax')\n",
        "        ])\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train(self, states, actions, advantages):\n",
        "        with tf.GradientTape() as tape:\n",
        "            probabilities = self.model(states)\n",
        "            actions_one_hot = tf.one_hot(actions, depth=self.action_dim)\n",
        "            action_probabilities = tf.reduce_sum(actions_one_hot * probabilities, axis=1)\n",
        "            log_probabilities = tf.math.log(action_probabilities)\n",
        "            loss = -tf.reduce_mean(log_probabilities * advantages)\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "class Critic:\n",
        "    def __init__(self, state_dim, learning_rate):\n",
        "        self.state_dim = state_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n",
        "\n",
        "    def create_model(self):\n",
        "        return tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(24, activation='relu', input_shape=(self.state_dim,)),\n",
        "            tf.keras.layers.Dense(24, activation='relu'),\n",
        "            tf.keras.layers.Dense(1)\n",
        "        ])\n",
        "\n",
        "    def predict(self, state):\n",
        "        return self.model.predict(state)\n",
        "\n",
        "    def train(self, states, td_targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            values = self.model(states)\n",
        "            loss = tf.reduce_mean(tf.square(td_targets - values))\n",
        "\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "class DDPG:\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "\n",
        "        self.actor_learning_rate = 0.001\n",
        "        self.critic_learning_rate = 0.01\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        self.actor = Actor(self.state_dim, self.action_dim, self.actor_learning_rate)\n",
        "        self.critic = Critic(self.state_dim, self.critic_learning_rate)\n",
        "\n",
        "    def get_action(self,state):\n",
        "      prob=self.actor.predict(state)[0]\n",
        "      action=np.random.choice(self.action_dim,p=prob)\n",
        "      return action\n",
        "\n",
        "    def train(self, states, actions,rewards,next_states,dones):\n",
        "      values=self.critic.predict(states)\n",
        "      next_values=self.critic.predict(next_states)\n",
        "      td_targets=rewards+self.gamma*next_values*(1-dones)\n",
        "      advantages=td_targets-values\n",
        "      actor_loss=self.actor.train(states,np.array(actions),advantages)\n",
        "      critic_loss=self.critic.train(states,np.array(td_targets))\n",
        "\n",
        "ddpg=DDPG(state_dim=state_dim ,action_dim=action_dim)\n",
        "\n",
        "num_episodes=1000\n",
        "total_rewards=[]\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  episode_reward=0\n",
        "  while not done:\n",
        "    action=ddpg.get_action(state.reshape(1,-1))\n",
        "    next_state,reward,done,_=env.step(action)\n",
        "    ddpg.train(state.reshape(1,-1),action,reward,next_state.reshape(1,-1),done)\n",
        "    episode_reward+=reward\n",
        "    state=next_state\n",
        "  total_rewards.append(episode_reward)\n",
        "  print('Episode = ', episode, ' | Reward = ', episode_reward)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(total_rewards)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()\n",
        "plt.savefig('DDPG_Cartpole.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L35XL6uzfeFa",
        "outputId": "feea9356-861b-4a96-c10f-ed6b52badf12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the environment for at least 10 episodes using only greedy actions from the learned policy and plot the total reward per episode.\n",
        "num_episodes_greedy=10\n",
        "total_rewards_greedy=[]\n",
        "\n",
        "for episode in range(num_episodes_greedy):\n",
        "  state=env.reset()\n",
        "  done=False\n",
        "  episode_reward=0\n",
        "  while not done:\n",
        "    prob=ddpg.actor.predict(state.reshape(1,-1))[0]\n",
        "    action=np.argmax(prob)\n",
        "    next_state,reward,done,_=env.step(action)\n",
        "    episode_reward+=reward\n",
        "    state=next_state\n",
        "  total_rewards_greedy.append(episode_reward)\n",
        "\n",
        "plt.plot(total_rewards_greedy)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.show()\n",
        "plt.savefig('DDPG_Cartpole_Testing.png')"
      ],
      "metadata": {
        "id": "zTtntDobmC_6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0osi4K6pJw4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
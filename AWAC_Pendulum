{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Input, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "4QTL_Ltbgid7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(Model):\n",
        "    def __init__(self, action_dim, max_action):\n",
        "        super().__init__()\n",
        "        self.max_action = max_action\n",
        "        self.fc1 = Dense(64, activation='relu')\n",
        "        self.fc2 = Dense(64, activation='relu')\n",
        "        self.mu = Dense(action_dim, activation='tanh')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.fc2(x)\n",
        "        mu = self.max_action * self.mu(x)\n",
        "        return mu\n",
        "\n",
        "\n",
        "class Critic(Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = Dense(64, activation='relu')\n",
        "        self.fc2 = Dense(64, activation='relu')\n",
        "        self.value = Dense(1, activation=None)\n",
        "\n",
        "    def call(self, state, action):\n",
        "        x = Concatenate()([state, action])\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        value = self.value(x)\n",
        "        return value\n"
      ],
      "metadata": {
        "id": "vrDExWsUgizp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AWACAgent:\n",
        "  def __init__(self, state_dim, action_dim, max_action, learning_rate=3e-4):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.max_action = max_action\n",
        "        self.actor_lr = learning_rate\n",
        "        self.critic_lr = learning_rate\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.buffer_size = 100000\n",
        "        self.batch_size = 256\n",
        "        self.actor = Actor(self.action_dim, self.max_action)\n",
        "        self.critic1 = Critic()\n",
        "        self.critic2 = Critic()\n",
        "        self.target_critic1 = Critic()\n",
        "        self.target_critic2 = Critic()\n",
        "        self.actor_optimizer = Adam(learning_rate=self.actor_lr)\n",
        "        self.critic1_optimizer = Adam(learning_rate=self.critic_lr)\n",
        "        self.critic2_optimizer = Adam(learning_rate=self.critic_lr)\n",
        "        self.buffer = {'state': np.zeros((self.buffer_size, self.state_dim)),\n",
        "                       'action': np.zeros((self.buffer_size, self.action_dim)),\n",
        "                       'reward': np.zeros((self.buffer_size, 1)),\n",
        "                       'next_state': np.zeros((self.buffer_size, self.state_dim)),\n",
        "                       'done': np.zeros((self.buffer_size, 1))}\n",
        "        self.pointer = 0\n",
        "\n",
        "  def act(self, state):\n",
        "      state = np.expand_dims(state, axis=0)\n",
        "      action = self.actor(state)[0]\n",
        "      noise = tf.random.normal(shape=action.shape, stddev=0.1)\n",
        "      action = action + noise\n",
        "      return np.clip(action, -self.max_action, self.max\n",
        "                       \n",
        "  def remember(self, state, action, reward, next_state, done):\n",
        "      self.buffer['state'][self.pointer] = state\n",
        "      self.buffer['action'][self.pointer] = action\n",
        "      self.buffer['reward'][self.pointer] = reward\n",
        "      self.buffer['next_state'][self.pointer] = next_state\n",
        "      self.buffer['done'][self.pointer] = done\n",
        "      self.pointer = (self.pointer + 1) % self.buffer_size\n",
        "\n",
        "  def sample(self):\n",
        "      indices = np.random.randint(0, self.buffer_size, size=self.batch_size)\n",
        "      state = self.buffer['state'][indices]\n",
        "      action = self.buffer['action'][indices]\n",
        "      reward = self.buffer['reward'][indices]\n",
        "      next_state = self.buffer['next_state'][indices]\n",
        "      done = self.buffer['done'][indices]\n",
        "      return state, action, reward, next_state, done\n",
        "\n",
        "  def update_critic(self, state, action, reward, next_state, done):\n",
        "      target_action = self.actor(next_state)\n",
        "      noise = tf.random.normal(shape=target_action.shape, stddev=0.2)\n",
        "      target_action = target_action + noise\n",
        "      target_action = tf.clip_by_value(target_action, -self.max_action, self.max_action)\n",
        "      target_value1 = self.target_critic1(next_state, target_action)\n",
        "      target_value2 = self.target_critic2(next_state, target_action)\n",
        "      target_value = tf.minimum(target_value1, target_value2)\n",
        "      target = reward + (1 - done) * self.gamma * target_value\n",
        "      with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "          value1 = self.critic1(state, action)\n",
        "          value2 = self.critic2(state, action)\n",
        "          loss1 = tf.keras.losses.MSE(target, value1)\n",
        "          loss2 = tf.keras.losses.MSE(target, value2)\n",
        "      grads1 = tape1.gradient(loss1, self.critic1.trainable_variables)\n",
        "      grads2 = tape2.gradient(loss2, self.critic2.trainable_variables)\n",
        "      self.critic1_optimizer.apply_gradients(zip(grads1, self.critic1.trainable_variables))\n",
        "      self.critic2_optimizer.apply_gradients(zip(grads2, self.critic2.trainable_variables))\n",
        "\n",
        "  def update_actor(self, state):\n",
        "      with tf.GradientTape() as tape:\n",
        "          action = self.actor(state)\n",
        "          loss = -tf.reduce_mean(self.critic1(state, action))\n",
        "      grads = tape.gradient(loss, self.actor.trainable_variables)\n",
        "      self.actor_optimizer.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
        "\n",
        "  def update_target_networks(self):\n",
        "      weights1 = []\n",
        "      targets1 = self.target_critic1.weights\n",
        "      for i, weight in enumerate(self.critic1.weights):\n",
        "          weights1.append(weight * self.tau + targets1[i] * (1 - self.tau))\n",
        "      self.target_critic1.set_weights(weights1)\n",
        "\n",
        "      weights2 = []\n",
        "      targets2 = self.target_critic2.weights\n",
        "      for i, weight in enumerate(self.critic2.weights):\n",
        "          weights2.append(weight * self.tau + targets2[i] * (1 - self.tau))\n",
        "      self.target_critic2.set_weights(weights2)\n",
        "\n",
        "  def train(self, env, episodes):\n",
        "      for i in range(episodes):\n",
        "          state = env.reset()\n",
        "          episode_reward = 0\n",
        "          done = False\n",
        "          while not done:\n",
        "              action = self.act(state)\n",
        "              next_state, reward, done, info = env.step(action)\n",
        "              self.remember(state, action, reward, next_state, done)\n",
        "              state = next\n",
        "              if len(self.buffer['state']) > self.batch_size:\n",
        "                  state, action, reward, next_state, done = self.sample()\n",
        "                  self.update_critic(state, action, reward, next_state, done)\n",
        "                  self.update_actor(state)\n",
        "                  self.update_target_networks()\n",
        "              episode_reward += reward\n",
        "          print(f\"Episode {i + 1}: Reward = {episode_reward:.2f}\")"
      ],
      "metadata": {
        "id": "4RTcbXZFgleg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gym\n",
        "\n",
        "# env = gym.make('Pendulum-v0')\n",
        "\n",
        "# agent = AWACAgent(env.observation_space.shape[0], env.action_space.shape[0])\n",
        "\n",
        "# agent.train(env, episodes=100)\n"
      ],
      "metadata": {
        "id": "4zbS3Q68hWve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbODPA4lhbN0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}